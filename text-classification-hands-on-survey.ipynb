{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-14T07:04:07.033221Z",
     "iopub.status.busy": "2021-10-14T07:04:07.032679Z",
     "iopub.status.idle": "2021-10-14T07:05:52.852776Z",
     "shell.execute_reply": "2021-10-14T07:05:52.851891Z",
     "shell.execute_reply.started": "2021-10-14T07:04:07.033170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow --upgrade\n",
    "#!pip install tf-models-official\n",
    "#!pip install tensorflow-text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "print(tf.__version__) # 2.6.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text and Document Feature Extraction\n",
    "### 1.1. Introduction to data preprocessing\n",
    "Data goes through a series of steps during preprocessing:\n",
    "   - **Data cleaning**: is the process of\n",
    "        + Detecting and correcting (removing) corrupt or inaccurate records from a record set, table or database\n",
    "        + Identifying incomplete, incorrect, inaccurate or irrelevant parts of data\n",
    "        + Replacing, modifying or deleting the dirty or coarse data. \n",
    "        \n",
    "   A common data cleansing practice is data enhancement, where data is made more complete by adding related information and data editing that is the process involving the review and adjustment of collected survey data, to control the quality of the collected data.\n",
    "    \n",
    "   - **Data wrangling**: Sometimes referred to as data munging, is the process of transforming and mapping data from one “raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. This may include further munging, data visualization, data aggregation, training a statistical model...\n",
    "   \n",
    "   - **Data reduction or instance/feature selection and extraction**: Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process and might improve the accuracy in classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Text Cleaning and Pre-processing\n",
    "Text preprocessing method is used to preprocess your text simply means to bring your text into a form that is predictable and analyzable for your task. Text preprocessing is task-specific.\n",
    "* **Must Do**:\n",
    "    - Noise removal\n",
    "    - Lowercasing (can be task dependent in some cases)\n",
    "* **Should Do**:\n",
    "    - Simple normalization – (e.g. standardize near identical words)\n",
    "* **Task Dependent**:\n",
    "    - Advanced normalization (e.g. addressing out-of-vocabulary words)\n",
    "    - Stop-word removal\n",
    "    - Stemming / Lemmatization\n",
    "    - Text enrichment / Augmentation\n",
    "* **Additional considerations**:\n",
    "    - Handling large documents and large collections of text documents that do not fit into memory.\n",
    "    - Extracting text from markup like HTML, PDF or other structured document formats.\n",
    "    - Trans-literation of characters from other languages into English.\n",
    "    - Handling of domain specific words, phrases and acronyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are examples of some techniques.\n",
    "- **Tokenization**: is the process of breaking down a stream of text into words, phrases, symbols, or any other meaningful elements called tokens. The main goal of this step is to extract individual words in a sentence.\n",
    "    \n",
    "    + You could use spaCy (or gensim, nltk). Recommend to use spaCy because it's one of the most versatile and widely used libraries in NLP.\n",
    "    + You could also use tf.text with BERTTokenizer for this tasks (and more - full pipeline in tensorflow)\n",
    "\n",
    "- **Stop words**: \n",
    "    In spaCy, you could get the complete list via:\n",
    "    `from spacy.lang.en.stop_words import STOP_WORDS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:05:52.862228Z",
     "iopub.status.busy": "2021-10-14T07:05:52.862019Z",
     "iopub.status.idle": "2021-10-14T07:05:55.115373Z",
     "shell.execute_reply": "2021-10-14T07:05:55.114695Z",
     "shell.execute_reply.started": "2021-10-14T07:05:52.862201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'is', 'the', 'hottest', 'day', 'of', 'this', 'month']\n",
      "['today', 'hottest', 'day', 'month']\n"
     ]
    }
   ],
   "source": [
    "import spacy # don't --> do and n't\n",
    "\n",
    "text = 'today is the hottest day of this month'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = nlp(text)\n",
    "token_list, filtered_sentence = [], []\n",
    "for token in tokens:\n",
    "    token_list.append(token.text)\n",
    "    if not token.is_stop: # you could also customize stop_words like this: nlp.vocab[word] = False\n",
    "        filtered_sentence.append(token.text)\n",
    "\n",
    "print(token_list)\n",
    "print(filtered_sentence)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Capitalization**: To reduce the problem space, the most common approach is to reduce everything to lower case. This brings all words in a document in same space, but it often changes the meaning of some words, such as \"US\" to \"us\" where first one represents the United States of America and second one is a pronoun. To solve this, slang and abbreviation converters can be applied.\n",
    "- **Slang and Abbreviations**: An abbreviation is a shortened form of a word, such as SVM stand for Support Vector Machine. Slang is a version of language that depicts informal conversation or text that has different meaning, such as \"lost the plot\", it essentially means that 'they've gone mad'. Common method to deal with these words is converting them to formal language.\n",
    "- **Noise removal**: remove punctuations or special characters - this is one of the most essential text preprocessing steps and also highly domain dependent. There are various ways to remove noise, it all depends on which domain you are working in and what entails noise for your task. This includes:\n",
    "    + punctuation removal\n",
    "    + special character removal\n",
    "    + numbers removal\n",
    "    + html formatting removal\n",
    "    + domain specific keyword removal (e.g. ‘RT’ for retweet)\n",
    "    + source code removal\n",
    "    + header removal and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:05:55.118081Z",
     "iopub.status.busy": "2021-10-14T07:05:55.117656Z",
     "iopub.status.idle": "2021-10-14T07:05:55.143332Z",
     "shell.execute_reply": "2021-10-14T07:05:55.142693Z",
     "shell.execute_reply.started": "2021-10-14T07:05:55.118051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?rt atuser the bigger the better multistop if you know what i mean . \"iran, with its unity and god is grace, will punch the arrogance (west) nd of bahman (feb ) in a way that will leave them stunned,\" wordsonobamashand do not say the n-word. atuser sold! would love to be your crazyass big sis -- how could i say no?! cannot believe i broke or minimally battered my toe -- i need money! i need new car multiexclamation  jesus multistop somebody please buy my old car ddd'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''},  # remove spaces at the beginning\n",
    "        {r'(\\\\u[0-9A-Fa-f]+)': r''}, # remove unidecode with null\n",
    "        {r'[^\\x00-\\x7f]': r''}, # remove unidecode with null\n",
    "        {r'((www\\.[^\\s]+)|(https?://[^\\s]+))': r'url'}, # replace url with \"url\"\n",
    "        {'@[^\\s]+': 'atUser'}, # replace @ with \"atUser\"\n",
    "        {r'#([^\\s]+)': r'\\1'}, # remove hashtag before word (\\1 mean group 1)\n",
    "        {r'(\\!)\\1+': ' multiExclamation '}, # replace multiple !\n",
    "        {r'(\\?)\\1+': ' multiQuestion '}, # replace multiple ?\n",
    "        {r'(\\.)\\1+': ' multiStop '}, # replace multiple .\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items(): # k = rule, v = output\n",
    "            text = re.sub(k, v, text) \n",
    "    # Remove number\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # Replaces contractions from a string to their equivalents\n",
    "    contraction_patterns = [\n",
    "        (r'won\\'t', 'will not'), \n",
    "        (r'can\\'t', 'cannot'), \n",
    "        (r'i\\'m', 'i am'), \n",
    "        (r'ain\\'t', 'is not'), \n",
    "        (r'(\\w+)\\'ll', '\\g<1> will'), \n",
    "        (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "        (r'(\\w+)\\'ve', '\\g<1> have'), \n",
    "        (r'(\\w+)\\'s', '\\g<1> is'), \n",
    "        (r'(\\w+)\\'re', '\\g<1> are'), \n",
    "        (r'(\\w+)\\'d', '\\g<1> would'), \n",
    "        (r'&', 'and'), \n",
    "        (r'dammit', 'damn it'), \n",
    "        (r'dont', 'do not'), \n",
    "        (r'wont', 'will not') \n",
    "    ]\n",
    "    for (pattern, repl) in contraction_patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "\n",
    "    # Remove Emoticons\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', \n",
    "                  '', text)\n",
    "    \n",
    "    return text.rstrip().lower()\n",
    "\n",
    "some_text_from_twitter = '''?RT @justinbiebcr: The bigger the better....if you know what I mean ;). \n",
    "\"Iran, with its unity and God's grace, will punch the arrogance (West) 22nd of Bahman (Feb 11) in a way that will leave them stunned,\"\n",
    "#4WordsOnObamasHand Don't Say The N-Word. @russmarshalek Sold! Would love to be your crazyass big sis -- how could I say no?! Cannot believe I broke or minimally battered my toe --\n",
    "i need money! i need new car!!! jesus...somebody please buy my old car :DDD'''\n",
    "\n",
    "text_cleaner(some_text_from_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spelling corretion**: such as hashing-based and context-sensitive spelling correction techniques, or spelling correction using trie and damerau-levenshtein distance bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:05:55.145410Z",
     "iopub.status.busy": "2021-10-14T07:05:55.144423Z",
     "iopub.status.idle": "2021-10-14T07:06:05.944572Z",
     "shell.execute_reply": "2021-10-14T07:06:05.943703Z",
     "shell.execute_reply.started": "2021-10-14T07:05:55.145379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "aaaaaa\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "message\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "service\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "!pip3 install autocorrect\n",
    "from autocorrect import spell \n",
    "\n",
    "print(spell(u'caaaar'))\n",
    "print(spell(u'mussage'))\n",
    "print(spell(u'survice'))\n",
    "print(spell(u'hte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stemming**: modifying a word to obtain its variants using different linguistic processeses like affixation (addition of affixes). E.g. changed, changes, changing, changer --> chang but not change as in Lemmatization, so it might not be useful as Lemmatization (but you also find it via nltk library)\n",
    "- **Lemmatization**: the process of eliminating redundant prefix or suffix of a word and extract the *base word* (lemma).\n",
    "\n",
    "More linguistic features (such as POS, is_alpha...) about NLP could be found at spaCy website: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:06:05.946330Z",
     "iopub.status.busy": "2021-10-14T07:06:05.945981Z",
     "iopub.status.idle": "2021-10-14T07:06:05.951596Z",
     "shell.execute_reply": "2021-10-14T07:06:05.950980Z",
     "shell.execute_reply.started": "2021-10-14T07:06:05.946288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'be', 'the', 'hot', 'day', 'of', 'this', 'month']\n"
     ]
    }
   ],
   "source": [
    "print([token.lemma_ for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some examples with Web data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:06:05.952791Z",
     "iopub.status.busy": "2021-10-14T07:06:05.952573Z",
     "iopub.status.idle": "2021-10-14T07:06:09.159379Z",
     "shell.execute_reply": "2021-10-14T07:06:09.158341Z",
     "shell.execute_reply.started": "2021-10-14T07:06:05.952766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " STATIC WEB \n",
      "\n",
      "<class 'str'> 1201520\n",
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsk\n",
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsk\n",
      "-------------------------------------------------- \n",
      " HTML \n",
      "\n",
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n",
      "['\\n\\n\\n', 'BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'\", 'to', 'die', 'out', 'in', '200', 'years', \"'\", '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'NEWS', '\\n\\xa0\\xa0', 'SPORT', '\\n\\xa0\\xa0', 'WEATHER', '\\n\\xa0\\xa0', 'WORLD', 'SERVICE', '\\n\\n\\xa0\\xa0', 'A', '-', 'Z', 'INDEX', '\\xa0\\n\\n\\xa0\\xa0', 'SEARCH', '\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n    \\xa0', 'You', 'are', 'in', ':', '\\xa0', 'Health', '\\xa0\\r\\n    \\r\\n    \\r\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'News', 'Front', 'Page', '\\n\\n\\n\\n\\n\\n', 'Africa', '\\n\\n\\n', 'Americas', '\\n\\n\\n', 'Asia', '-', 'Pacific']\n",
      "\n",
      "\n",
      "\n",
      "BBC NEWS | Health | Blondes 'to die out in 200 \n",
      "-------------------------------------------------- \n",
      " BLOG \n",
      "\n",
      "Language Log\n",
      "13\n",
      "Saliva chicken\n",
      "<p>We've alluded to this Sichuanese dish in posts and comments several\n",
      "We've alluded to this Sichuanese dish in posts and comments several times before on Language, but this is the first time I have captured it in the wild (at Nan Zhou Hand Drawn Noodle House in Philadelphia's Chinatown):\n",
      "\n",
      "kǒushuǐ 口水 (\"mouth + water\") = \"saliva\"\n",
      "liú kǒushuǐ 流口水 (\"flow + mouth + water\") = \"slobber; slaver; salivate; drool; dribble; drivel\"\n",
      "kǒushuǐ jī 口水鸡 (\"mouth + water + chicken\") = \"mouthwatering chicken\", i.e., \"steamed chicken with chili sauce\".\n",
      "I will try out this peculiarly named dish the next time I go to Chinatown and see if it lives up to its sensational name.\n",
      "Selected readings\n",
      "\n",
      "\"Great taste\" (5/20/14)\n",
      "\"Chinese Japanese\" (9/13/15)\n",
      "\"Bean crud\" (1/9/14)\n",
      "\n",
      "[Thanks to Zihan Guo]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# a. Reading data from static web\n",
    "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
    "raw = requests.get(url).text\n",
    "\n",
    "print('-'*50, '\\n', 'STATIC WEB', '\\n')\n",
    "print(type(raw), len(raw))\n",
    "print(raw[:75])\n",
    "\n",
    "# New decoding\n",
    "new_r = raw.encode().decode('utf-8-sig')\n",
    "print(new_r[:75])\n",
    "\n",
    "# b. Dealing with HTML\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = requests.get(url).text\n",
    "print('-'*50, '\\n', 'HTML', '\\n')\n",
    "print(html[:60])\n",
    "\n",
    "# Get text out of HTML\n",
    "from bs4 import BeautifulSoup # !pip install beautifulsoup4 \n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = [token.text for token in nlp(raw)]\n",
    "print(tokens[:50])\n",
    "print(raw[:50])\n",
    "\n",
    "# + Deal with blogosphere data\n",
    "import feedparser # !pip install feedparser\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "print('-'*50, '\\n', 'BLOG', '\\n')\n",
    "print(llog['feed']['title'])\n",
    "print(len(llog.entries))\n",
    "\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "\n",
    "content = post.content[0].value\n",
    "print(content[:70])\n",
    "\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Others: Extracting Text from PDF, MSWord and other Binary Formats**\n",
    "\n",
    "ASCII text and HTML text are human readable formats. Text often comes in binary formats — like PDF and MSWord — that can only be opened using specialized software. Third-party libraries such as **pypdf** and **pywin32** provide access to these formats. Extracting text from multi-column documents is particularly challenging. For once-off conversion of a few documents, it is simpler to open the document with a suitable application, then save it as text to your local drive, and access it as described below. If the document is already on the web, you can enter its URL in Google's search box. The search result often includes a link to an HTML version of the document, which you can save as text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Word Embedding\n",
    "Different word embedding procedures have been proposed to translate these unigrams into consummable input for machine learning algorithms. A very simple way to perform such embedding is term-frequency~(TF) where each word will be mapped to a number corresponding to the number of occurrence of that word in the whole corpora. The other term frequency functions have been also used that represent word-frequency as Boolean or logarithmically scaled number. Here, each document will be converted to a vector of same length containing the frequency of the words in that document. Although such approach may seem very intuitive but it suffers from the fact that particular words that are used very commonly in language literature might dominate this sort of word representations.\n",
    "\n",
    "- **Word2Vec**: original & reference from https://code.google.com/p/word2vec/\n",
    "\n",
    " Given a text corpus, the word2vec tool learns a vector for every word in the vocabulary using the Continuous Bag-of-Words or the Skip-Gram neural network architectures. \n",
    "\n",
    " The user should specify the following: desired vector dimensionality (size of the context window for either the Skip-Gram or the Continuous Bag-of-Words model), training algorithm (hierarchical softmax and / or negative sampling) or threshold for downsampling the frequent words, number of threads to use, format of the output word vector file (text or binary).\n",
    "\n",
    "- **GLoVe**: Global Vectors for Word Representation\n",
    "\n",
    " An implementation of the GloVe model for learning word representations is provided, and describe how to download web-dataset vectors or train your own. See the project page at https://nlp.stanford.edu/projects/glove/ for more information on glove vectors.\n",
    "\n",
    "- **Contextualized Word Representations**\n",
    "\n",
    " ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.\n",
    "\n",
    " ELMo representations are:\n",
    "  - Contextual: The representation for each word depends on the entire context in which it is used.\n",
    "  - Deep: The word representations combine all layers of a deep pre-trained neural network.\n",
    "  - Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training.\n",
    "\n",
    "  You could use ELMo via two ways:\n",
    "   - Easy version at https://tfhub.dev/google/elmo/2\n",
    "   - Some other pretrained models: https://allennlp.org/elmo - including a JSON formatted \"options\" file with hyperparameters and a hdf5 formatted file with the model weights. There are three ways to integrate ELMo representations into a downstream task, depending on your use case.\n",
    "    - Compute representations on the fly from raw text using character input. This is the most general method and will handle any input text. It is also the most computationally expensive. This method is necessary for evaluating at test time on unseen data (e.g. public SQuAD leaderboard)\n",
    "    - Precompute and cache the context independent token representations, then compute context dependent representations using the biLSTMs for input data. This method is less computationally expensive then #1, but is only applicable with a fixed, prescribed vocabulary. This approach is a good compromise for large datasets where the size of the file in is unfeasible (SNLI, SQuAD)\n",
    "    - Precompute the representations for your entire dataset and save to a file.\n",
    "It is a good choice for smaller datasets or in cases where you'd like to use ELMo in other frameworks.\n",
    "\n",
    "   In all cases, the process roughly follows the same steps. First, create a Batcher (or TokenBatcher for #2) to translate tokenized strings to numpy arrays of character (or token) ids. Then, load the pretrained ELMo model (class BidirectionalLanguageModel). Finally, for steps #1 and #2 use weight_layers to compute the final ELMo representations. For #3, use BidirectionalLanguageModel to write all the intermediate layers to a file.\n",
    "\n",
    "* **FastText**: fastText is a library for efficient learning of word representations and sentence classification. Reference at https://github.com/facebookresearch/fastText and https://fasttext.cc/docs/en/english-vectors.html\n",
    " - Features: \n",
    "   - Recent state-of-the-art English word vectors.\n",
    "   - Word vectors for 157 languages trained on Wikipedia and Crawl.\n",
    "   - Models for language identification and various supervised tasks.\n",
    "\n",
    "### 1.4. Weighted Words\n",
    "- Term frequency is Bag of words that is one of the simplest techniques of text feature extraction. This method is based on counting number of the words in each document and assign it to feature space.\n",
    "- Term Frequency-Inverse Document Frequency\n",
    "Although tf-idf tries to overcome the problem of common terms in document (through lowering weight by divide the number of documents contain these terms), it still suffers from some other descriptive limitations. Namely, tf-idf cannot account for the **similarity between words** in the document since each word is presented as an index. In the recent years, with development of more complex models, such as neural nets, new methods has been presented that can **incorporate concepts**, such as **similarity of words** and **part of speech tagging**. This work uses, **word2vec and Glove**, two of the most common methods that have been successfully used for deep learning techniques.\n",
    "\n",
    "### 1.5. Comparison of Feature Extraction Techniques\n",
    "Reference more at https://github.com/kk7nc/Text_Classification#comparison-of-feature-extraction-techniques\n",
    "\n",
    "There are 4 main performance problems with these techniques:\n",
    " - Syntactic (capture the position in the text): TF-IDF could not solve this\n",
    " - Semantics (capture meaning in the text): TF-IDF could not solve this\n",
    " - Polysemy (meaning of the word from the text - context): only ELMo (Contextualized Word Representations) could solve this\n",
    " - Out-of-vocabulary words from corpus: FastText and TF-IDF support this via n-gram hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text classification techniques\n",
    "\n",
    "* Data preparation - IMDB dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:06:09.161129Z",
     "iopub.status.busy": "2021-10-14T07:06:09.160849Z",
     "iopub.status.idle": "2021-10-14T07:07:18.939237Z",
     "shell.execute_reply": "2021-10-14T07:07:18.938290Z",
     "shell.execute_reply.started": "2021-10-14T07:06:09.161095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = tfds.load('imdb_reviews', split='train', batch_size=-1, as_supervised=True)\n",
    "X_test, y_test = tfds.load('imdb_reviews', split='test', batch_size=-1, as_supervised=True)\n",
    "# batch_size=-1 for full dataset in single batch\n",
    "\n",
    "raw_train_ds = tfds.load('imdb_reviews', split='train', as_supervised=True).take(1000) \n",
    "raw_test_ds = tfds.load('imdb_reviews', split='test', as_supervised=True).take(1000)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Non-neural network techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:07:18.940910Z",
     "iopub.status.busy": "2021-10-14T07:07:18.940666Z",
     "iopub.status.idle": "2021-10-14T07:07:33.605736Z",
     "shell.execute_reply": "2021-10-14T07:07:33.604617Z",
     "shell.execute_reply.started": "2021-10-14T07:07:18.940879Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=1000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train.numpy()).toarray() # toarray used for later use in DNN training\n",
    "X_test_tfidf = vectorizer.transform(X_test.numpy()).toarray()\n",
    "\n",
    "def benchmark(clf, nn=False, embedding=True, train_data=raw_train_ds, test_data=raw_test_ds, n_epochs=5):\n",
    "    if nn:\n",
    "        if not embedding:\n",
    "            clf.fit(X_train_tfidf, y_train, validation_data=(X_test_tfidf, y_test), epochs=n_epochs, batch_size=128)\n",
    "            pred = (clf.predict(X_test_tfidf) > 0.5).astype('int32')\n",
    "            print('Accuracy: ', round(metrics.accuracy_score(y_test, pred), 2))\n",
    "        else:\n",
    "            clf.fit(train_data, validation_data=test_data, epochs=n_epochs, batch_size=128)\n",
    "            print('Loss, Accuracy: ', clf.evaluate(test_data))\n",
    "    else:\n",
    "        clf.fit(X_train_tfidf, y_train)\n",
    "        pred = clf.predict(X_test_tfidf)\n",
    "        \n",
    "        accuracy = round(metrics.accuracy_score(y_test, pred), 2)\n",
    "        clf_descr = str(clf).split('(')[0]\n",
    "        return clf_descr, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:07:33.609168Z",
     "iopub.status.busy": "2021-10-14T07:07:33.608884Z",
     "iopub.status.idle": "2021-10-14T07:11:00.556726Z",
     "shell.execute_reply": "2021-10-14T07:11:00.555334Z",
     "shell.execute_reply.started": "2021-10-14T07:07:33.609133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_clf = [\n",
    "            MultinomialNB(alpha=.01),\n",
    "            LinearSVC(penalty='l2', dual=False, tol=1e-3),\n",
    "            RidgeClassifier(tol=1e-2, solver=\"auto\"),\n",
    "            SGDClassifier(alpha=.0001, max_iter=50, penalty='l2'),\n",
    "            GradientBoostingClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "]\n",
    "\n",
    "results = [benchmark(clf) for clf in list_clf]\n",
    "\n",
    "# Aggregate results\n",
    "indices = np.arange(len(results))\n",
    "update_results = [[x[i] for x in results] for i in range(2)] # convert to list of lists\n",
    "\n",
    "clf_names, accuracy = update_results\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, accuracy, .2, label=\"accuracy\", color='navy')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Deep Neural Networks\n",
    "Deep Neural Networks architectures are designed to learn through multiple connection of layers where each single layer only receives connection from previous and provides connections only to the next layer in hidden part.\n",
    "\n",
    "The input is a connection of feature space (as feature_extraction with first hidden layer). For Deep Neural Networks (DNN), input layer could be tf-idf, word embedding or etc.\n",
    "The output layer houses neurons equal to the number of classes for multi-class classification and only one neuron for binary classification.\n",
    "\n",
    "- Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:11:00.559219Z",
     "iopub.status.busy": "2021-10-14T07:11:00.558876Z",
     "iopub.status.idle": "2021-10-14T07:11:10.154972Z",
     "shell.execute_reply": "2021-10-14T07:11:10.152753Z",
     "shell.execute_reply.started": "2021-10-14T07:11:00.559174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.4674 - accuracy: 0.7839 - val_loss: 0.3326 - val_accuracy: 0.8541\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 0.3336 - accuracy: 0.8655 - val_loss: 0.3304 - val_accuracy: 0.8524\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 0.3110 - accuracy: 0.8728 - val_loss: 0.3268 - val_accuracy: 0.8543\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 0.2844 - accuracy: 0.8841 - val_loss: 0.3339 - val_accuracy: 0.8520\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 0.2451 - accuracy: 0.9020 - val_loss: 0.3604 - val_accuracy: 0.8501\n",
      "Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "def create_dnn(): # without embedding layer to compare with previous results\n",
    "    model = keras.models.Sequential()\n",
    "    # model.add(keras.Input(shape=(X_train_tfidf.shape[1],)))\n",
    "    # or model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "    # or input_shape=[None, 1] in the next layers (without batch_size)\n",
    "    model.add(keras.layers.Dense(64, input_dim=1000, activation='relu'))\n",
    "\n",
    "    n_layers = 2\n",
    "    for i in range(0, n_layers):\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(0.5))    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model \n",
    "\n",
    "model = create_dnn()\n",
    "benchmark(model, nn=True, embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:11:10.158115Z",
     "iopub.status.busy": "2021-10-14T07:11:10.157637Z",
     "iopub.status.idle": "2021-10-14T07:11:10.229015Z",
     "shell.execute_reply": "2021-10-14T07:11:10.227734Z",
     "shell.execute_reply.started": "2021-10-14T07:11:10.158080Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "# Model constants\n",
    "max_features = 1000\n",
    "embedding_dim = 128\n",
    "sequence_length = 200\n",
    "\n",
    "def word_embedding():\n",
    "    \n",
    "    # Convert text to word embedding\n",
    "    def custom_standardization(X_batch):\n",
    "        X_batch = tf.strings.lower(X_batch)\n",
    "        X_batch = tf.strings.regex_replace(X_batch, \"<br />\", \" \")\n",
    "        # Some more preprocessing\n",
    "        #X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "        #X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "        #X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "        #X_batch = tf.strings.split(X_batch)\n",
    "        #return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "        return tf.strings.regex_replace(X_batch, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode=\"int\", # normalize, split, and map strings to integers\n",
    "        output_sequence_length=sequence_length, # explicit sequence length, since the CNNs won't support ragged sequences\n",
    "    )\n",
    "    # vectorizer transforms strings into vocabulary indices\n",
    "\n",
    "    text_ds = raw_train_ds.map(lambda x, y: x) # make a text-only dataset (no labels)\n",
    "    vectorizer.adapt(text_ds) # call `adapt` to create the vocabulary. \n",
    "    # Don't have to batch, but for very large datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "    # Two options to vectorize the data\n",
    "    # (+) Option 1: Make it part of the model, so as to obtain a model that processes raw strings, like this:\n",
    "    #text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    #x = vectorize_layer(text_input)\n",
    "    #x = layers.Embedding(max_features + 1, embedding_dim)(x) \n",
    "    #...\n",
    "    # (+) Option 2: Apply it to the text dataset to obtain a dataset of word indices, then feed it into a model that \n",
    "    # expects integer sequences as inputs. This  enables you to do asynchronous CPU processing and buffering of your data when training on GPU.\n",
    "    def vectorize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        return vectorizer(text), [label]\n",
    "\n",
    "    # Vectorize the data. Do async prefetching / buffering of the data for best performance on GPU.\n",
    "    train_ds = raw_train_ds.map(vectorize_text).cache().prefetch(buffer_size=10)\n",
    "    # or vectorizer(np.array([[s] for s in train_samples])).numpy() for raw_text\n",
    "    test_ds = raw_test_ds.map(vectorize_text).cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    return train_ds, test_ds, vectorizer\n",
    "\n",
    "def word_pretrain_embedding(path_to_glove_file, vectorizer):\n",
    "    \n",
    "    # Convert text to word embedding (Using pre-trained word-embedding GloVe)\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file) as f: # path_to_glove_file - e.g. glove.6B.100d.txt\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "    # prepare a corresponding embedding matrix that we can use in a Keras Embedding layer\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    print([word_index[w] for w in test])\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim)) # max_features or len(word_index) + len(oov)\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        max_features,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix), # adding this compare to no pre-trained model\n",
    "        trainable=False, #  keep the embeddings fixed (we don't want to update them during training)\n",
    "    )\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "train_ds, test_ds, vectorizer = word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For pretraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T07:11:10.229856Z",
     "iopub.status.idle": "2021-10-14T07:11:10.230184Z",
     "shell.execute_reply": "2021-10-14T07:11:10.230032Z",
     "shell.execute_reply.started": "2021-10-14T07:11:10.230016Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip # 822 MB\n",
    "#!unzip -q glove.6B.zip\n",
    "#path_to_glove_file = os.path.join(os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\")\n",
    "#pretrained_embedding_layer = word_pretrain_embedding(path_to_glove_file=, vectorizer=vectorizer)\n",
    "#model = create_cnn(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T07:11:10.232559Z",
     "iopub.status.idle": "2021-10-14T07:11:10.233082Z",
     "shell.execute_reply": "2021-10-14T07:11:10.232844Z",
     "shell.execute_reply.started": "2021-10-14T07:11:10.232819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6977 - accuracy: 0.4760 - val_loss: 0.6924 - val_accuracy: 0.5160\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6943 - accuracy: 0.5340 - val_loss: 0.6699 - val_accuracy: 0.6240\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6452 - accuracy: 0.6400 - val_loss: 0.6499 - val_accuracy: 0.6390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.4639 - accuracy: 0.7980 - val_loss: 0.6395 - val_accuracy: 0.6900\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.3056 - accuracy: 0.8810 - val_loss: 0.8048 - val_accuracy: 0.6830\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.1680 - accuracy: 0.9540 - val_loss: 0.9613 - val_accuracy: 0.6970\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.1243 - accuracy: 0.9530 - val_loss: 1.2305 - val_accuracy: 0.6950\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.0812 - accuracy: 0.9730 - val_loss: 1.4205 - val_accuracy: 0.7090\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0946 - accuracy: 0.9690 - val_loss: 1.2258 - val_accuracy: 0.6880\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0599 - accuracy: 0.9840 - val_loss: 1.6558 - val_accuracy: 0.6880\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.6558 - accuracy: 0.6880\n",
      "Loss, Accuracy:  [1.6558232307434082, 0.6880000233650208]\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(pretrained=False, n_filters=64, kernel_size=5):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(None,), dtype=\"int64\")) # A integer input for vocab indices.\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
    "    if not pretrained: \n",
    "        model.add(keras.layers.Embedding(max_features, embedding_dim))\n",
    "    else:\n",
    "        model.add(pretrained_embedding_layer)\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    n_layers = 2\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Conv1D(n_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=2))\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_cnn()\n",
    "benchmark(model, nn=True, embedding=True, train_data=train_ds, test_data=test_ds, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN: \n",
    "\n",
    "    Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. For more information about implementation of stateful RNN, check at https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T07:11:10.234573Z",
     "iopub.status.idle": "2021-10-14T07:11:10.235089Z",
     "shell.execute_reply": "2021-10-14T07:11:10.234852Z",
     "shell.execute_reply.started": "2021-10-14T07:11:10.234827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " RNN: \n",
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 151s 149ms/step - loss: 0.7150 - accuracy: 0.4890 - val_loss: 0.7023 - val_accuracy: 0.5030\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 0.6992 - accuracy: 0.5520 - val_loss: 0.6964 - val_accuracy: 0.5150\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 140s 140ms/step - loss: 0.6509 - accuracy: 0.5820 - val_loss: 0.7710 - val_accuracy: 0.5210\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.7710 - accuracy: 0.5210\n",
      "Loss, Accuracy:  [0.7709929347038269, 0.5210000276565552]\n",
      "-------------------------------------------------- \n",
      " GRU: \n",
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 674s 667ms/step - loss: 0.6978 - accuracy: 0.4880 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 676s 676ms/step - loss: 0.6958 - accuracy: 0.5070 - val_loss: 0.6944 - val_accuracy: 0.4980\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 649s 649ms/step - loss: 0.6807 - accuracy: 0.5610 - val_loss: 0.7486 - val_accuracy: 0.4830\n",
      "1000/1000 [==============================] - 67s 67ms/step - loss: 0.7486 - accuracy: 0.4830\n",
      "Loss, Accuracy:  [0.748552143573761, 0.4830000102519989]\n"
     ]
    }
   ],
   "source": [
    "def create_rnn(pretrained=False):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(None,), dtype=\"int64\")) # A integer input for vocab indices.\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
    "    if not pretrained: \n",
    "        model.add(keras.layers.Embedding(max_features, embedding_dim))\n",
    "    else:\n",
    "        model.add(pretrained_embedding_layer)\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # RNN here!!!\n",
    "    model.add(keras.layers.SimpleRNN(32, return_sequences=True))\n",
    "    model.add(keras.layers.SimpleRNN(32))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_gru(pretrained=False):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(None,), dtype=\"int64\")) # A integer input for vocab indices.\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
    "    if not pretrained: \n",
    "        model.add(keras.layers.Embedding(max_features, embedding_dim))\n",
    "    else:\n",
    "        model.add(pretrained_embedding_layer)\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # GRU / LSTM\n",
    "    n_layers = 2\n",
    "    for i in range(n_layers):\n",
    "        model.add(keras.layers.GRU(32, return_sequences=True, recurrent_dropout=0.2)) \n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(keras.layers.GRU(32, recurrent_dropout=0.2)) # the last layer do not return sequence\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "print('-'*50, '\\n', 'RNN: ')\n",
    "benchmark(create_rnn(), nn=True, embedding=True, train_data=train_ds, test_data=test_ds, n_epochs=3)\n",
    "\n",
    "print('-'*50, '\\n', 'GRU: ')\n",
    "benchmark(create_gru(), nn=True, embedding=True, train_data=train_ds, test_data=test_ds, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RCNN:\n",
    "\n",
    "    Recurrent Convolutional Neural Networks (RCNN) is also used for text classification. The main idea of this technique is capturing contextual information with the recurrent structure and constructing the representation of text using a convolutional neural network. This architecture is a combination of RNN and CNN to use advantages of both technique in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T04:56:22.166232Z",
     "iopub.status.busy": "2021-10-14T04:56:22.165955Z",
     "iopub.status.idle": "2021-10-14T04:59:32.735722Z",
     "shell.execute_reply": "2021-10-14T04:59:32.734251Z",
     "shell.execute_reply.started": "2021-10-14T04:56:22.166202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 46s 41ms/step - loss: 0.6954 - accuracy: 0.4770 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.6942 - accuracy: 0.5030 - val_loss: 0.6896 - val_accuracy: 0.5040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6466 - accuracy: 0.6360 - val_loss: 0.5904 - val_accuracy: 0.7240\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.4039 - accuracy: 0.8210 - val_loss: 0.5360 - val_accuracy: 0.7340\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2015 - accuracy: 0.9250 - val_loss: 0.5272 - val_accuracy: 0.7440\n",
      "1000/1000 [==============================] - 8s 7ms/step - loss: 0.5272 - accuracy: 0.7440\n",
      "Loss, Accuracy:  [0.5271626710891724, 0.7440000176429749]\n"
     ]
    }
   ],
   "source": [
    "def create_rcnn(pretrained=False, n_filters=32, kernel_size=3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(None,), dtype=\"int64\")) # A integer input for vocab indices.\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
    "    if not pretrained: \n",
    "        model.add(keras.layers.Embedding(max_features, embedding_dim))\n",
    "    else:\n",
    "        model.add(pretrained_embedding_layer)\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    n_layers = 2\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Conv1D(n_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=2))\n",
    "        model.add(keras.layers.MaxPooling1D(pool_size=2)) # move GlobalMaxPooling layers down\n",
    "    # Global max pooling layer return a fixed-length output vector for each example by averaging over the sequence dimension. \n",
    "    # This allows the model to handle input of variable length, in the simplest way possible.\n",
    "    # Global max pooling = ordinary max pooling layer with pool size equals to the size of the input \n",
    "    # (minus filter size + 1, to be precise)    \n",
    "    \n",
    "    # GRU / LSTM\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.LSTM(32, return_sequences=True, recurrent_dropout=0.2))\n",
    "        model.add(keras.layers.Dropout(0.5)) #or drop_out=0.5 in the previous layers\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_rcnn()\n",
    "benchmark(create_rcnn(), nn=True, embedding=True, train_data=train_ds, test_data=test_ds, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Export an end-to-end model**\n",
    "\n",
    "We may want to export a Model object that takes as input a string of arbitrary length, rather than a sequence of indices. It would make the model much more portable, since you wouldn't have to worry about the input preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T05:03:10.45399Z",
     "iopub.status.idle": "2021-10-14T05:03:10.454528Z",
     "shell.execute_reply": "2021-10-14T05:03:10.454252Z",
     "shell.execute_reply.started": "2021-10-14T05:03:10.454226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "indices = vectorizer(inputs)\n",
    "outputs = model(indices) # model = create_cnn().fit(train_ds, validation_data=test_ds, n_epochs=5)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Specific case\n",
    "prob = end_to_end_model.predict(\n",
    "    [[\"the worst movie ever\"]]\n",
    ")\n",
    "\n",
    "prob.argmax(axis=-1) # class name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T05:02:37.72872Z",
     "iopub.status.busy": "2021-10-14T05:02:37.727473Z",
     "iopub.status.idle": "2021-10-14T05:03:00.987127Z",
     "shell.execute_reply": "2021-10-14T05:03:00.985661Z",
     "shell.execute_reply.started": "2021-10-14T05:02:37.728647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
      "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.7626287   0.9928098  -0.18611833  0.3667385   0.1523368   0.65504456\n",
      "  0.9681154  -0.9486271   0.00216172 -0.987773    0.06842685 -0.97630596]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.28946376  0.3432126   0.3323155  ...  0.21300781  0.7102078\n",
      "  -0.05771124]\n",
      " [-0.2874204   0.31981033 -0.23018603 ...  0.58455133 -0.21329767\n",
      "   0.7269206 ]\n",
      " [-0.66157013  0.68876886 -0.8743305  ...  0.10877283 -0.26173198\n",
      "   0.47855327]\n",
      " ...\n",
      " [-0.22561157 -0.28925607 -0.07064402 ...  0.47566003  0.83277047\n",
      "   0.40025362]\n",
      " [-0.29824257 -0.2747315  -0.05450503 ...  0.4884973   1.0955352\n",
      "   0.18163386]\n",
      " [-0.44378242  0.00930739  0.07223728 ...  0.17290103  1.1833243\n",
      "   0.07897934]]\n",
      "tf.Tensor([[0.56900954]], shape=(1, 1), dtype=float32)\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# a. Pretrained model with BERT (Tensorflow Hub)\n",
    "# Reference more at: https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "map_name_to_handle = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "}\n",
    "map_model_to_preprocess = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}') # the input is truncated to 128 tokens - could be customized\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}') # all zero for single sentence input. \n",
    "# For a multiple sentence input, it would have one number for each input\n",
    "\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "# Pooled Output: each input sequence as a whole. The shape is [batch_size, H] (as an embedding for the entire movie review)\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "# Sequence Output: each input token in the context. The shape is [batch_size, seq_length, H]. \n",
    "# You can think of this as a contextual embedding for every token in the movie review.\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\n",
    "\n",
    "def create_model_tfhub():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Could check that the model runs with the output of the preprocessing model.\n",
    "model = create_model_tfhub()\n",
    "bert_raw_result = model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "# For looking at model's architecture\n",
    "#!pip install pydot graphviz\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T05:06:50.291187Z",
     "iopub.status.busy": "2021-10-14T05:06:50.290798Z",
     "iopub.status.idle": "2021-10-14T05:24:23.347826Z",
     "shell.execute_reply": "2021-10-14T05:24:23.346261Z",
     "shell.execute_reply.started": "2021-10-14T05:06:50.291147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 118s 3s/step - loss: 7.4308 - accuracy: 0.5120 - val_loss: 7.6704 - val_accuracy: 0.4970\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 110s 3s/step - loss: 7.4721 - accuracy: 0.5100 - val_loss: 7.6704 - val_accuracy: 0.4970\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 112s 4s/step - loss: 7.4721 - accuracy: 0.5100 - val_loss: 7.6704 - val_accuracy: 0.4970\n",
      "32/32 [==============================] - 26s 811ms/step - loss: 7.6704 - accuracy: 0.4970\n",
      "Loss, Accuracy:  [7.67036771774292, 0.4970000088214874]\n"
     ]
    }
   ],
   "source": [
    "# pretrained model requires batched data -- the accuracy is very low since I've just take only 1000 samples\n",
    "update_train_ds = raw_train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "update_test_ds = raw_test_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "benchmark(create_model_tfhub(), nn=True, embedding=True, train_data=update_train_ds, test_data=update_test_ds, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cells will take a bit longer to run and the result accuracy is approximately 0.5, so I just introduce this implementation and ignore to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T04:59:32.73838Z",
     "iopub.status.busy": "2021-10-14T04:59:32.738056Z",
     "iopub.status.idle": "2021-10-14T05:02:10.552257Z",
     "shell.execute_reply": "2021-10-14T05:02:10.549988Z",
     "shell.execute_reply.started": "2021-10-14T04:59:32.738315Z"
    }
   },
   "outputs": [],
   "source": [
    "# b. Transformers\n",
    "# for Switch Transformers, reference at https://keras.io/examples/nlp/text_classification_with_switch_transformer/\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.models.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "embedding_dim = 128\n",
    "num_heads = 2 # number of attention head\n",
    "ff_dim = 32 # hidden layer size in feed forward network inside transformer\n",
    "\n",
    "def create_transformers(pretrained=False):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(None,), dtype=\"int64\")) # A integer input for vocab indices.\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
    "    if not pretrained: \n",
    "        model.add(keras.layers.Embedding(max_features, embedding_dim))\n",
    "    else:\n",
    "        model.add(pretrained_embedding_layer)\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # Transformers\n",
    "    model.add(TransformerBlock(embedding_dim, num_heads, ff_dim))\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")) # == Dense(2, activation=\"softmax\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) # sparse_categorical_crossentropy\n",
    "\n",
    "    return model\n",
    "\n",
    "benchmark(create_transformers(), nn=True, embedding=False, train_data=train_ds, test_data=test_ds, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T05:03:10.448576Z",
     "iopub.status.idle": "2021-10-14T05:03:10.449035Z",
     "shell.execute_reply": "2021-10-14T05:03:10.448848Z",
     "shell.execute_reply.started": "2021-10-14T05:03:10.448828Z"
    }
   },
   "outputs": [],
   "source": [
    "# c. Another implementation from transformers repository\n",
    "# For finetuning, reference at https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb\n",
    "!pip install transformers\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T05:03:10.451435Z",
     "iopub.status.idle": "2021-10-14T05:03:10.452077Z",
     "shell.execute_reply": "2021-10-14T05:03:10.451786Z",
     "shell.execute_reply.started": "2021-10-14T05:03:10.451757Z"
    }
   },
   "outputs": [],
   "source": [
    "# For inference\n",
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "model.save(saved_model_path, include_optimizer=False)\n",
    "\n",
    "reloaded_model = tf.saved_model.load(saved_model_path)\n",
    "def print_my_examples(inputs, results):\n",
    "    result_for_printing = [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}' for i in range(len(inputs))]\n",
    "    print(*result_for_printing, sep='\\n')\n",
    "    print()\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information to optimize performance, coding..., reference at https://www.tensorflow.org/guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Comparison Text classification models\n",
    "* Non-neural network:\n",
    "    - Pros:\n",
    "        + Work well with small dataset\n",
    "        + Computational complexity is cheap & faster\n",
    "        + Implementation is much easier\n",
    "        + Interpretability: available, but might loss with some complex algorithms such as bagging, boosting\n",
    "    - Cons:\n",
    "        + Scalability: small-scale\n",
    "* Deep Learning:\n",
    "    - Pros:\n",
    "        + Flexible with features design (reduces the need for feature engineering, one of the most time-consuming parts of machine learning practice)\n",
    "        + Architecture that can be adapted to new problems\n",
    "        + Can deal with complex input-output mappings: End-to-End architecture\n",
    "        + Can easily handle online learning (It makes it very easy to re-train the model when newer data becomes available)\n",
    "        + Parallel processing capability (It can perform more than one job at the same time)\n",
    "    - Cons:\n",
    "        + Requires a large amount of data (if you only have small sample text data, deep learning is unlikely to outperform other approaches)\n",
    "        + Problem: complex input-output mappings\n",
    "        + Extremely computationally expensive to train\n",
    "        + Model interpretability is hard\n",
    "        + Finding an efficient architecture and structure is still the main challenge of this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Others\n",
    "### 3.1. Data downloading & preparation\n",
    "Exploit libraries: `keras.utils & pathlib`. E.g. download the Newsgroup20 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-14T05:03:10.456453Z",
     "iopub.status.idle": "2021-10-14T05:03:10.456977Z",
     "shell.execute_reply": "2021-10-14T05:03:10.456727Z",
     "shell.execute_reply.started": "2021-10-14T05:03:10.456701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['comp.graphics', 'talk.religion.misc', 'rec.autos', 'comp.windows.x', 'sci.electronics', 'comp.os.ms-windows.misc', 'talk.politics.misc', 'comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware', 'rec.motorcycles', 'misc.forsale', 'sci.crypt', 'talk.politics.mideast', 'sci.med', 'sci.space', 'soc.religion.christian', 'rec.sport.baseball', 'alt.atheism', 'rec.sport.hockey', 'talk.politics.guns']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['39664', '39065', '38896', '39000', '38503']\n",
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")\n",
    "\n",
    "# Let's take a look at the data\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])\n",
    "\n",
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read()) # very helpful with pathlib lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-06T03:45:24.809901Z",
     "iopub.status.idle": "2021-10-06T03:45:24.810513Z",
     "shell.execute_reply": "2021-10-06T03:45:24.810258Z",
     "shell.execute_reply.started": "2021-10-06T03:45:24.810226Z"
    }
   },
   "source": [
    "### 3.2. Some Applications\n",
    "* **Information Retrieval**\n",
    "\n",
    "Information retrieval is finding documents of an unstructured data that meet an information need from within large collections of documents. With the rapid growth of online information, particularly in text format, text classification has become a significant technique for managing this type of data. Some of the important methods used in this area are Naive Bayes, SVM, decision tree, J48, k-NN and IBK. One of the most challenging applications for document and text dataset processing is applying document categorization methods for information retrieval.\n",
    "\n",
    "* **Information Filtering**\n",
    "\n",
    "Information filtering refers to selection of relevant information or rejection of irrelevant information from a stream of incoming data. Information filtering systems are typically used to measure and forecast users' long-term interests. Probabilistic models, such as Bayesian inference network, are commonly used in information filtering systems. Bayesian inference networks employ recursive inference to propagate values through the inference network and return documents with the highest ranking. Chris used vector space model with iterative refinement for filtering task.\n",
    "\n",
    "* **Sentiment Analysis**\n",
    "\n",
    "Sentiment analysis is a computational approach toward identifying opinion, sentiment, and subjectivity in text. Sentiment classification methods classify a document associated with an opinion to be positive or negative. The assumption is that document d is expressing an opinion on a single entity e and opinions are formed via a single opinion holder h. Naive Bayesian classification and SVM are some of the most popular supervised learning methods that have been used for sentiment classification. Features such as terms and their respective frequency, part of speech, opinion words and phrases, negations and syntactic dependency have been used in sentiment classification techniques.\n",
    "\n",
    "* **Recommender Systems**\n",
    "\n",
    "Content-based recommender systems suggest items to users based on the description of an item and a profile of the user's interests. A user's profile can be learned from user feedback (history of the search queries or self reports) on items as well as self-explained features~(filter or conditions on the queries) in one's profile. In this way, input to such recommender systems can be semi-structured such that some attributes are extracted from free-text field while others are directly specified. Many different types of text classification methods, such as decision trees, nearest neighbor methods, Rocchio's algorithm, linear classifiers, probabilistic methods, and Naive Bayes, have been used to model user's preference.\n",
    "\n",
    "* **Knowledge Management**\n",
    "\n",
    "Textual databases are significant sources of information and knowledge. A large percentage of corporate information (nearly 80 %) exists in textual data formats (unstructured). In knowledge distillation, patterns or knowledge are inferred from immediate forms that can be semi-structured ( e.g.conceptual graph representation) or structured/relational data representation). A given intermediate form can be document-based such that each entity represents an object or concept of interest in a particular domain. Document categorization is one of the most common methods for mining document-based intermediate forms. In the other work, text classification has been used to find the relationship between railroad accidents' causes and their correspondent descriptions in reports.\n",
    "\n",
    "* **Document Summarization**\n",
    "\n",
    "Text classification used for document summarizing which summary of a document may employ words or phrases which do not appear in the original document. Multi-document summarization also is necessitated due to increasing online information rapidly. So, many researchers focus on this task using text classification to extract important feature out of a document."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
